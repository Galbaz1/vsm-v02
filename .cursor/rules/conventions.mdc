---
alwaysApply: true
---

# Conventions

## Stack
- **Backend**: Python 3.12, FastAPI, Weaviate 1.34
- **Frontend**: Next.js 16, React 19, Tailwind v4
- **Inference**: MLX (target), Ollama (embeddings only)
- **Models**: Qwen3 (routing/synthesis), Qwen2.5-VL (vision), ColQwen2.5-v0.2
- **Env**: Conda `vsm-hva`

## Structure
```
api/
├── main.py           # Entry
├── core/config.py    # Settings
├── schemas/          # Pydantic
├── services/         # Logic (search.py, colqwen.py, agent.py, tools/)
└── endpoints/        # Thin routes only
scripts/              # Run from root: python scripts/foo.py
static/previews/      # Page PNGs
static/manuals/       # PDFs
data/                 # Source PDFs, parsed JSON, benchmarks
```

## Naming
- Python: `snake_case` (files, funcs, vars)
- Classes: `PascalCase`
- Constants: `UPPER_SNAKE_CASE`
- TypeScript: `camelCase` vars, `PascalCase` components

## Collections
| Collection | Vectorizer | Dim | Use |
|------------|-----------|-----|-----|
| `AssetManual` | nomic-embed-text | 768 | Text chunks |
| `PDFDocuments` | colqwen (named) | 1024×128 | Page images |

**Never delete collections** - append mode, check exists first.

## Ports
- API: 8001
- Frontend: 3000
- Weaviate: 8080
- Ollama: 11434

## Commands
```bash
# Infrastructure
docker compose up -d                    # Weaviate + Ollama
uvicorn api.main:app --reload --port 8001
cd frontend && npm run dev

# MLX (target inference)
pip install mlx mlx-lm
mlx_lm.server --model mlx-community/Qwen2.5-7B-Instruct-4bit --port 8000

# Test
python scripts/run_benchmark.py
curl "http://localhost:8001/search?query=voltage"
```

## Rules
1. Logic in services, not endpoints
2. Run scripts from project root
3. Use `with weaviate.connect_to_local()` context manager
4. Set `PYTORCH_ENABLE_MPS_FALLBACK=1` for ColQwen
5. Never commit `.env` files
6. Update ARCHITECTURE.md for schema/API changes
7. Prefer MLX over Ollama for LLM inference (Apple Silicon native)
8. Use Elysia patterns: Environment state, Tool availability, Error objects
