---
alwaysApply: true
---

# Conventions

The current date is 2025-11-26. --always ensure you use this date when writing code or documentation, or when doing online research.

## Stack
- **Backend**: Python 3.12, FastAPI (Streaming NDJSON), Weaviate 1.34
- **Frontend**: Next.js 16, React 19, Tailwind v4
- **Inference**: Native Ollama (gpt-oss), MLX (VLM only)
- **Env**: Conda `vsm-hva`

## Models
| Role | Model | Runtime | Memory |
|---|----|---|-----|
| LLM (decision + tools) | `gpt-oss:120b` | Native Ollama | ~65GB |
| Embeddings | `bge-m3` | Native Ollama | ~1.2GB |
| Vision | `Qwen3-VL-8B` | MLX | ~8GB |
| Visual Retrieval | `ColQwen2.5-v0.2` | PyTorch | ~4GB |

## Architecture
```
┌─────────────────────────────────────────────────────────────┐
│                    Mac Studio (Host)                        │
│  ┌─────────────────┐    ┌─────────────────────────────────┐│
│  │  Native Ollama  │    │  API (8001) / Frontend (3000)   ││
│  │  (0.0.0.0:11434)│    └─────────────────────────────────┘│
│  │  - gpt-oss:120b │                                       │
│  │  - bge-m3       │    host.docker.internal:11434         │
│  └────────┬────────┘              ▼                        │
│           │           ┌─────────────────┐                  │
│           └──────────▶│ Weaviate (8080) │ Docker           │
│                       └─────────────────┘                  │
└─────────────────────────────────────────────────────────────┘
```

**Critical**: Weaviate runs in Docker, connects to Native Ollama via `host.docker.internal:11434`. Never use Dockerized Ollama (can't access full RAM/GPU).

## Structure
```
api/
├── main.py              # FastAPI entry
├── core/                # Config & initialization
├── schemas/             # Pydantic models
├── services/            # Business logic (Agent, LLM, Search)
│   ├── agent.py         # Decision tree orchestrator
│   └── tools/           # Tool implementations
└── endpoints/           # Thin routes only
scripts/
├── weaviate_ingest_manual.py # Text ingestion
├── colqwen_ingest.py         # Visual ingestion
└── ...
static/previews/         # Page PNGs
```

## Collections
| Collection | Vectorizer | Use |
|---|-----|-----|
| `AssetManual` | bge-m3 (8K context) | Text chunks |
| `PDFDocuments` | ColQwen2.5 | Page images (multi-vector) |

## Ports
| Service | Port |
|---|---|
| API | 8001 |
| Frontend | 3000 |
| Weaviate | 8080 |
| Native Ollama | 11434 |
| MLX VLM | 8000 |

## Commands
```bash
./scripts/start.sh          # Start all services
./scripts/stop.sh           # Stop all services
ollama pull gpt-oss:120b    # Pull LLM
ollama pull bge-m3          # Pull embeddings
```

## Rules
1. **Clean Arch**: Logic in `services/`, thin `endpoints/`, models in `schemas/`.
2. **Streaming**: API responses must be NDJSON/SSE (don't break this).
3. Run scripts from project root
4. Use `with weaviate.connect_to_local()` context manager
5. Weaviate ingestion: `api_endpoint="http://host.docker.internal:11434"`
6. Single LLM (gpt-oss) for decision + complex; MLX only for VLM
7. Tools yield `Result` with `llm_message` for decision agent context

## ⚠️ Debugging Agent Issues

**When user reports a query failing/looping, DON'T read raw files. Use the analyzer:**

```bash
# 1. Find trace
ls logs/query_traces/

# 2. Run intelligent analysis (Gemini 3 Pro reads EVERYTHING)
python scripts/analyze_with_llm.py --gemini-only <trace_id_prefix>
```

This loads entire codebase + traces into Gemini (1M context) and returns concise diagnosis with exact file:line fix.

**NEVER**: `cat logs/`, `grep` through data files, read trace JSON manually.
**ALWAYS**: Use the analyzer script - it keeps your context clean.

## Custom Commands

When user types a command, check `.cursor/commands/` for instructions:
- `/debug` → Quick debugging workflow
- `/analyze` → **Analysis Agent**: Research issues, create diagnosis file (NO code edits)
- `/implement` → **Coding Agent**: Apply fixes from analysis file (NO research)

## Progress Tracking
See `.cursor/rules/progress.mdc` for recent fixes and known issues.
