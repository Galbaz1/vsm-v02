---
alwaysApply: true
---

# Conventions

## Stack
- **Backend**: Python 3.12, FastAPI, Weaviate 1.34
- **Frontend**: Next.js 16, React 19, Tailwind v4
- **Inference**: Ollama (gpt-oss), MLX (VLM only)
- **Env**: Conda `vsm-hva`

## Models (Final)
| Role | Model | Runtime | Active Params | Memory |
|------|-------|---------|---------------|--------|
| Decision Agent | `gpt-oss:120b` | Ollama | 5.1B (MoE) | ~65GB |
| Complex Tools | `gpt-oss:120b` | Ollama | 5.1B (MoE) | shared |
| Vision | `Qwen3-VL-8B` | MLX | 8B | ~8GB |
| Retrieval | `ColQwen2.5-v0.2` | PyTorch | 2B | ~4GB |
| Embeddings | `nomic-embed-text` | Ollama | - | ~2GB |

**Why gpt-oss-120B**: Elysia-recommended, MoE (fast), agentic-trained, single model for both roles.

## Structure
```
api/
├── main.py           # Entry
├── core/config.py    # Settings
├── schemas/          # Pydantic (agent.py has Result/Error)
├── services/         # Logic
│   ├── agent.py      # Decision tree orchestrator
│   ├── environment.py # Centralized state
│   ├── search.py     # FastVector (AssetManual)
│   ├── colqwen.py    # ColQwen (PDFDocuments)
│   └── tools/        # Tool implementations
└── endpoints/        # Thin routes only
scripts/              # Run from root
static/previews/      # Page PNGs
static/manuals/       # PDFs
```

## Naming
- Python: `snake_case` (files, funcs, vars)
- Classes: `PascalCase`
- Constants: `UPPER_SNAKE_CASE`

## Collections
| Collection | Vectorizer | Use |
|------------|-----------|-----|
| `AssetManual` | nomic-embed-text | Text chunks |
| `PDFDocuments` | ColQwen2.5 | Page images (multi-vector) |

**Never delete collections** - append mode, check exists first.

## Ports
| Service | Port |
|---------|------|
| API | 8001 |
| Frontend | 3000 |
| Weaviate | 8080 |
| Ollama | 11434 |
| MLX Server | 8000 |

## Commands
```bash
# Infrastructure
docker compose up -d                    # Weaviate + Ollama
ollama pull gpt-oss:120b                # Decision/Complex LM
uvicorn api.main:app --reload --port 8001

# VLM (MLX)
pip install mlx mlx-vlm
mlx_vlm.server --model mlx-community/Qwen3-VL-8B-Instruct-4bit --port 8000

# Test
curl "http://localhost:8001/search?query=voltage"
curl -N "http://localhost:8001/agentic_search?query=show+diagram"
```

## Rules
1. Logic in services, not endpoints
2. Run scripts from project root
3. Use `with weaviate.connect_to_local()` context manager
4. Set `PYTORCH_ENABLE_MPS_FALLBACK=1` for ColQwen
5. Never commit `.env` files
6. Update ARCHITECTURE.md for schema/API changes
7. Single LLM (gpt-oss) for decision + complex; MLX only for VLM
8. Use Elysia patterns: Environment, Tool base, Result/Error objects
9. Tools yield `Result` with `llm_message` for decision agent context
