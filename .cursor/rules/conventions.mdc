---
alwaysApply: true
---

# Conventions

The current date is 2025-11-26. --always ensure you use this date when writing code or documentation, or when doing online research.

## Stack
- **Backend**: Python 3.12, FastAPI (Streaming NDJSON), Weaviate 1.34
- **Frontend**: Next.js 16, React 19, Tailwind v4
- **Inference**: Mode-switchable via `VSM_MODE` env var
- **Env**: Conda `vsm-hva`

## Models (by Mode)
| Role | Local | Cloud |
|------|-------|-------|
| LLM | `gpt-oss:120b` (Ollama) | Gemini 2.5 Flash |
| VLM | `Qwen3-VL-8B` (MLX) | Gemini 2.5 Flash |
| Embeddings | `bge-m3` (Ollama) | Jina v4 |
| Visual Retrieval | ColQwen2.5-v0.2 | **Serverless Worker** (Jina v4) |

## Architecture
```
┌─────────────────────────────────────────────────────────────┐
│                    Mac Studio (Host)                        │
│  ┌─────────────────┐    ┌─────────────────────────────────┐│
│  │  Native Ollama  │    │  API (8001) / Frontend (3000)   ││
│  │  (0.0.0.0:11434)│    └─────────────────────────────────┘│
│  │  - gpt-oss:120b │                                       │
│  │  - bge-m3       │    host.docker.internal:11434         │
│  └────────┬────────┘              ▼                        │
│           │           ┌─────────────────┐                  │
│           └──────────▶│ Weaviate (8080) │ Docker           │
│                       └─────────────────┘                  │
└─────────────────────────────────────────────────────────────┘
```

**Critical**: Weaviate runs in Docker, connects to Native Ollama via `host.docker.internal:11434`. Never use Dockerized Ollama (can't access full RAM/GPU).

## Structure
```
api/
├── main.py              # FastAPI entry
├── core/
│   ├── config.py        # Settings + VSM_MODE
│   └── providers/       # LLM/VLM/Embed/VectorDB abstractions
├── schemas/             # Pydantic models
├── services/            # Business logic (Agent, Search)
│   ├── agent.py         # Decision tree orchestrator
│   └── tools/           # Tool implementations (use providers!)
├── prompts/             # DSPy signatures + compiled modules
│   ├── local/           # Optimized for gpt-oss
│   └── cloud/           # Optimized for Gemini
└── endpoints/           # Thin routes only
scripts/
├── weaviate_ingest_manual.py # Text ingestion
├── cloud_ingest.py           # Cloud ingestion (pending)
└── ...
docs/cloud-migration/    # Architecture docs
```

## Collections
| Collection | Vectorizer | Use |
|---|-----|-----|
| `AssetManual` | bge-m3 (8K context) | Text chunks |
| `PDFDocuments` | ColQwen2.5 | Page images (multi-vector) |

## Ports
| Service | Port |
|---|---|
| API | 8001 |
| Frontend | 3000 |
| Weaviate | 8080 |
| Native Ollama | 11434 |
| MLX VLM | 8000 |

## Commands
```bash
./scripts/start.sh          # Start all services
./scripts/stop.sh           # Stop all services
ollama pull gpt-oss:120b    # Pull LLM
ollama pull bge-m3          # Pull embeddings
```

## Rules
1. **Clean Arch**: Logic in `services/`, thin `endpoints/`, models in `schemas/`.
2. **Streaming**: API responses must be NDJSON/SSE (don't break this).
3. Run scripts from project root
4. **Provider Pattern**: Use `get_llm()`, `get_vlm()`, `get_embeddings()`, `get_vectordb()` - never direct clients
5. Weaviate ingestion (local): `api_endpoint="http://host.docker.internal:11434"`
6. Tools yield `Result` with `llm_message` for decision agent context
7. **Mode-agnostic tools**: Tools call providers, providers handle local/cloud

## ⚠️ Debugging Agent Issues

**When user reports a query failing/looping, DON'T read raw files. Use the analyzer:**

```bash
# 1. Find trace
ls logs/query_traces/

# 2. Run intelligent analysis (Gemini 3 Pro reads EVERYTHING)
python scripts/analyze_with_llm.py --gemini-only <trace_id_prefix>
```

This loads entire codebase + traces into Gemini (1M context) and returns concise diagnosis with exact file:line fix.

**NEVER**: `cat logs/`, `grep` through data files, read trace JSON manually.
**ALWAYS**: Use the analyzer script - it keeps your context clean.

## Custom Commands

When user types a command, check `.cursor/commands/` for instructions:
- `/debug` → Quick debugging workflow
- `/analyze` → **Analysis Agent**: Research issues, create diagnosis file (NO code edits)
- `/implement` → **Coding Agent**: Apply fixes from analysis file (NO research)

## Progress Tracking
See `.cursor/rules/progress.mdc` for recent fixes and known issues.
